from helpers.executable_api import ExecutableApi, Unbuffered
from database.output import data

import csv
import os, os.path
import sqlite3
import re
import sys
import traceback
from pathlib import Path
from datetime import datetime

def clean_column_name(name):
	"""Clean column names to be valid SQL identifiers."""
	name = name.strip()
	name = re.sub(r'[^\w]+', '_', name)
	name = name.strip('_')
	if name and name[0].isdigit():
		name = 'col_' + name
	return name.lower()

def infer_sql_type(value):
	"""Infer SQL data type from a string value."""
	value = value.strip()
	
	try:
		int(value)
		return 'INTEGER'
	except ValueError:
		pass
	
	try:
		float(value)
		return 'REAL'
	except ValueError:
		pass
	
	return 'TEXT'

def adjust_gis_id(gis_id_value, name_value):
	"""
	Adjust gis_id if it's 0 by extracting digits from name.
	
	Args:
		gis_id_value: Current gis_id value
		name_value: Value from the 'name' column
	
	Returns:
		Adjusted gis_id value
	"""
	try:
		if int(gis_id_value) == 0:
			subbed = re.sub('[^0-9]', '', str(name_value))
			if subbed.isdigit():
				return int(subbed)
			else:
				return 0
		return int(gis_id_value)
	except (ValueError, TypeError):
		return gis_id_value

def reset_database(db_file):
	"""Completely reset a SQLite database."""
	conn = sqlite3.connect(db_file)
	cursor = conn.cursor()
	
	# Get all objects
	cursor.execute("""
		SELECT type, name FROM sqlite_master 
		WHERE name NOT LIKE 'sqlite_%'
		ORDER BY type DESC
	""")
	objects = cursor.fetchall()
	
	# Drop in correct order (tables last)
	for obj_type, obj_name in objects:
		if obj_type == 'table':
			cursor.execute(f"DROP TABLE IF EXISTS {obj_name}")
		elif obj_type == 'index':
			cursor.execute(f"DROP INDEX IF EXISTS {obj_name}")
		elif obj_type == 'view':
			cursor.execute(f"DROP VIEW IF EXISTS {obj_name}")
		elif obj_type == 'trigger':
			cursor.execute(f"DROP TRIGGER IF EXISTS {obj_name}")
	
	conn.commit()
	cursor.execute("VACUUM")
	conn.close()

class ReadOutput(ExecutableApi):
	def __init__(self, output_files_dir, db_file, swat_version, editor_version, project_name, skip_files=[], batch_size=100000):
		self.__abort = False
		db_file_sanitized = db_file.replace("\\","/")
		try:
			os.remove(db_file_sanitized)
		except:
			reset_database(db_file_sanitized)

		self.output_files_dir = output_files_dir.replace("\\","/")
		self.swat_version = swat_version
		self.editor_version = editor_version
		self.project_name = project_name
		self.skip_files = skip_files
		self.batch_size = batch_size
		self.conn = sqlite3.connect(db_file_sanitized)
		self.cursor = self.conn.cursor()
	
	def __del__(self):
		self.conn.close()

	def read(self):	
		self.set_safety_level('fastest')	
		self.setup_meta_tables()

		files_out_file = os.path.join(self.output_files_dir, 'files_out.out')

		# Read files_out.out to get list of output CSV files
		files = []
		try:
			with open(files_out_file, "r") as file:
				i = 1
				for line in file:
					if i > 1:
						val = line.split()
						if len(val) < 2:
							raise ValueError('Unexpected number of columns in {}'.format(files_out_file))

						file_name = val[len(val)-1].strip()
						if file_name.endswith('.csv'):
							files.append(file_name)

					i += 1
		except FileNotFoundError:
			pass
		except ValueError as ve:
			sys.exit(ve)

		log_file_name = 'output_db_log.txt'
		log_file = os.path.join(self.output_files_dir, log_file_name)
		files_with_errors = []

		# Clear/create the log file
		with open(log_file, 'w') as f:
			f.write('SWAT+ Editor tries to read all .csv files listed in files.out generated by the model after a successful run. \n')
			f.write('Some messages, such as "No data found" may occur if you checked to write output files that do not really apply in your simulation. \n')
			f.write('Other errors may occur due to formatting issues and bugs in writing the output that SWAT+ Editor is unaware of. \n')
			f.write('In these cases, a detailed stack trace is available below and should be sent to the development team at the SWAT+ Editor Google Group: https://groups.google.com/g/swatplus-editor \n\n')

		prog_step = 0 if len(files) < 1 else round(100 / len(files))
		prog = 0
		log_issue_count = 0
		for file in files:
			if file in self.skip_files:
				continue
			self.emit_progress(prog, 'Importing {}...'.format(file))
			try:
				warnings = self.read_file(file)
				if warnings:
					with open(log_file, 'a') as f:
						f.write(warnings + '\n')
						log_issue_count += 1
			except Exception as e:
				with open(log_file, 'a') as f:
					f.write('Error processing {}:\n {}\n{}\n\n'.format(file, e, traceback.format_exc()))
					log_issue_count += 1
				files_with_errors.append(file)
			prog += prog_step

		if log_issue_count < 1:
			with open(log_file, 'a') as f:
				f.write('Import completed with no issues in {} files.\n'.format(len(files)))
					
		self.cursor.execute("""
			INSERT INTO project_config (project_name, editor_version, swat_version, output_import_time, skip_files)
			VALUES (?, ?, ?, ?, ?)
		""", (self.project_name, self.editor_version, self.swat_version, datetime.now(), ','.join(self.skip_files)))
		self.conn.commit()
		self.conn.close()

	def read_file(self, file):
		table_name = file[:-4].replace('hru-lte', 'hru_lte')
		desc_key = table_name
		time_series_key = ''
		for key in data.time_series_labels:
			desc_key = desc_key.replace('_{}'.format(key), '')
			if key in table_name:
				time_series_key = key

		description = '{ts} {n}'.format(ts=data.time_series_labels.get(time_series_key, ''), n=data.table_labels.get(desc_key, table_name))
		self.cursor.execute('INSERT INTO table_description (table_name, description) VALUES (?, ?)', (table_name, description))

		data_start_line = data.special_start_lines.get(desc_key, data.default_start_line)
		read_units = False if desc_key in data.ignore_units else True
		headings_start_line = data_start_line - 2 if read_units else data_start_line - 1

		cat_cols = None
		table_cat = data.table_categories.get(desc_key, None)
		if table_cat is not None:
			cat_cols = data.category_descriptions.get(table_cat, None)

		file_path = os.path.join(self.output_files_dir, file)
		check_path = Path(file_path)
		if not check_path.is_file():
			return 'File not found: {}'.format(file_path)

		# Read column headers, units (if applicable), and first data line for type inference
		with open(file_path, 'r') as f:
			i = 1
			while i < headings_start_line:
				f.readline()
				i += 1
			
			column_headers_raw = f.readline()
			null_counter = 1
			column_headers = []
			seen_columns = {}

			for col in column_headers_raw.split(','):
				cleaned = clean_column_name(col)
				
				# Handle "null" columns
				if cleaned == 'null':
					column_headers.append(f'undefined{null_counter}')
					null_counter += 1
				else:
					# Handle duplicate columns
					if cleaned in seen_columns:
						seen_columns[cleaned] += 1
						column_headers.append(f'{cleaned}{seen_columns[cleaned]}')
					else:
						seen_columns[cleaned] = 0
						column_headers.append(cleaned)

			units = []
			if read_units:
				units_line = f.readline()
				units = [unit.strip() for unit in units_line.split(',')]

			# First data row for type inference
			first_data_line = f.readline()

		# Parse first data row
		first_data_row = list(csv.reader([first_data_line]))[0]
		delimiter = ','
		if len(first_data_row) == 1:
			# Try tab delimiter
			first_data_row_tab = first_data_line.strip().split('\t')
			if len(first_data_row_tab) > 1:
				delimiter = '\t'
				first_data_row = first_data_row_tab
			else:
				# Try whitespace (one or more spaces)
				first_data_row = re.split(r'\s+', first_data_line.strip())
				delimiter = 'whitespace'  # Special marker, not a regex
		
		# Find indices for gis_id and name columns if fix_gis_id is enabled
		gis_id_idx = None
		name_idx = None
		fix_gis_id = True
		if fix_gis_id:
			try:
				gis_id_idx = column_headers.index('gis_id')
				name_idx = column_headers.index('name')
			except ValueError:
				fix_gis_id = False

		# Handle missing column names - check if we have fewer headers than data columns
		if len(column_headers) < len(first_data_row):
			unknown_counter = 1
			while len(column_headers) < len(first_data_row):
				column_headers.append(f'unknown{unknown_counter}')
				unknown_counter += 1
		
		# Infer data types from first data row
		column_types = [infer_sql_type(value) for value in first_data_row]
		
		# Create table schema
		columns_with_types = [f"{col_name} {col_type}" for col_name, col_type in zip(column_headers, column_types)]
		if not columns_with_types:
			return 'No data found in file: {}'.format(file_path)
		
		create_table_sql = f"""
		CREATE TABLE IF NOT EXISTS {table_name} (
			id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT,
			{', '.join(columns_with_types)}
		)
		"""	
		try:		
			self.cursor.execute(create_table_sql)
		except Exception as e:
			raise Exception('Error creating table {} SQL: {}'.format(table_name, create_table_sql))

		# Insert column descriptions (only for columns with units)
		column_desc_count = 0
		for col_name, unit in zip(column_headers, units):
			# Only insert if unit is not empty
			if unit and unit.strip():
				description = cat_cols.get(col_name, '') if cat_cols else ''
				self.cursor.execute("""
					INSERT INTO column_description (table_name, column_name, units, description)
					VALUES (?, ?, ?, ?)
				""", (table_name, col_name, unit.strip(), description))
				column_desc_count += 1
		
		self.conn.commit()

		# Prepare insert statement
		placeholders = ','.join(['?' for _ in column_headers])
		insert_sql = f"INSERT INTO {table_name} ({','.join(column_headers)}) VALUES ({placeholders})"

		# Process file in streaming fashion
		inserted_count = 0
		adjusted_count = 0
		batch = []
		
		with open(file_path, 'r', buffering=8192*1024) as f:  # 8MB buffer
			if delimiter == 'whitespace':
				# Custom generator for whitespace-delimited files
				reader = (re.split(r'\s+', line.strip()) for line in f)
			else:
				# Standard csv.reader for comma or tab (single character delimiters)
				reader = csv.reader(f, delimiter=delimiter)

			# Skip header lines
			i = 0
			for row in reader:
				i += 1
				if i < data_start_line or not row or len(row) != len(column_headers):
					continue
				
				# Adjust gis_id if needed (before type conversion)
				if fix_gis_id and gis_id_idx is not None and name_idx is not None:
					gis_id_val = row[gis_id_idx].strip()
					name_val = row[name_idx].strip()
					
					try:
						if gis_id_val and int(gis_id_val) == 0:
							adjusted_val = adjust_gis_id(gis_id_val, name_val)
							if adjusted_val != 0:
								row[gis_id_idx] = str(adjusted_val)
								adjusted_count += 1
					except (ValueError, TypeError):
						pass
				
				# Convert values - minimal processing for speed
				converted_row = []
				for value, col_type in zip(row, column_types):
					value = value.strip()
					if not value:
						converted_row.append(None)
					elif col_type == 'INTEGER':
						try:
							converted_row.append(int(value))
						except ValueError:
							converted_row.append(None)
					elif col_type == 'REAL':
						try:
							converted_row.append(float(value))
						except ValueError:
							converted_row.append(None)
					else:
						converted_row.append(value)
				
				batch.append(converted_row)
				
				# Batch insert for efficiency
				if len(batch) >= self.batch_size:
					self.cursor.executemany(insert_sql, batch)
					self.conn.commit()
					inserted_count += len(batch)
					batch = []
			
			# Insert remaining rows
			if batch:
				self.cursor.executemany(insert_sql, batch)
				self.conn.commit()
				inserted_count += len(batch)
		
		return None

	def setup_meta_tables(self):
		self.cursor.execute("""
			CREATE TABLE IF NOT EXISTS column_description (
				id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT,
				table_name VARCHAR(255) NOT NULL,
				column_name VARCHAR(255) NOT NULL,
				units VARCHAR(255),
				description VARCHAR(255)
			)
		""")

		self.cursor.execute("""
			CREATE TABLE IF NOT EXISTS table_description (
				table_name  VARCHAR (255) NOT NULL PRIMARY KEY,
				description VARCHAR (255) NOT NULL
			)
		""")

		self.cursor.execute("""
			CREATE TABLE IF NOT EXISTS project_config (
				id INTEGER NOT NULL PRIMARY KEY,
				project_name VARCHAR (255),
				editor_version VARCHAR (255),
				swat_version VARCHAR (255),
				output_import_time DATETIME,
				skip_files VARCHAR (255)
			)
		""")
		self.conn.commit()
	
	def set_safety_level(self, safety_level):
		# Set PRAGMA based on safety level
		if safety_level == 'safe':
			self.cursor.execute("PRAGMA journal_mode = WAL")
			self.cursor.execute("PRAGMA synchronous = NORMAL")
			self.cursor.execute("PRAGMA cache_size = -2000000")
			self.cursor.execute("PRAGMA temp_store = MEMORY")
			
		elif safety_level == 'balanced':
			self.cursor.execute("PRAGMA journal_mode = MEMORY")
			self.cursor.execute("PRAGMA synchronous = OFF")
			self.cursor.execute("PRAGMA cache_size = -2000000")
			self.cursor.execute("PRAGMA temp_store = MEMORY")
			
		else:  # fastest
			self.cursor.execute("PRAGMA journal_mode = OFF")
			self.cursor.execute("PRAGMA synchronous = OFF")
			self.cursor.execute("PRAGMA cache_size = -2000000")
			self.cursor.execute("PRAGMA temp_store = MEMORY")
			#self.cursor.execute("PRAGMA locking_mode = EXCLUSIVE")